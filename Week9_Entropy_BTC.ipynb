{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sp_stats\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLONIEX_OHLCV_BASEURL = 'https://poloniex.com/public?command=returnChartData&currencyPair='\n",
    "\n",
    "\"\"\"\n",
    "https://poloniex.com/public?command=returnChartData&currencyPair=BTC_POT&start=1435699200&end=9999999999&period=14400\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_ohlcv_poloniex(pair='USDT_BTC', start=1435699200, end=9999999999, period=900):\n",
    "    \"\"\"\n",
    "    returns ohlcv data for poloniex as pandas dataframe\n",
    "    convert to unix timestamp using https://coderstoolbox.net/unixtimestamp/\n",
    "    :param pair: str pair on poloniex\n",
    "    :param start: int unix timestamp of beginning time\n",
    "    :param end: int unix timestamp of ending time\n",
    "    :param period: int candle width in seconds\n",
    "    :return: pandas df of ohlcv data from poloniex for specified pair, times, and period\n",
    "    \"\"\"\n",
    "    query = POLONIEX_OHLCV_BASEURL + pair + '&start=' + str(start) + '&end=' + str(end) + '&period=' + str(period)\n",
    "    resp = requests.get(query,verify=False)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise requests.ApiError('GET /tasks/ {}'.format(resp.status_code))\n",
    "\n",
    "    return pd.DataFrame(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileEncoder:\n",
    "    \"\"\"\n",
    "    Performs quantile encoding for a real valued source.  The assigned value\n",
    "    for each bins is chosen as to minimize the mean square error introduced\n",
    "    by the quantization (conditional expectation within the bin).\n",
    "    Quantizer bins and mappings are computed from the empirical distribution\n",
    "    of the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, alphabet_size=2):\n",
    "        assert alphabet_size >= 2\n",
    "\n",
    "        self._quantiles = [n / float(alphabet_size)\n",
    "                           for n in range(alphabet_size + 1)]\n",
    "        self._bins = None\n",
    "        self._bin_mappings = None\n",
    "        self._suffix = '_right'\n",
    "\n",
    "    def fit(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Fit the quantizer parameters from training data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.fit_transform(data)\n",
    "\n",
    "    def transform(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Previously fit encoder performs the transformation on new data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self._bins is not None\n",
    "        assert self._bin_mappings is not None\n",
    "\n",
    "        ser = pd.cut(data, self._bins, labels=False)\n",
    "        ser.name = 'labels'\n",
    "\n",
    "        df = pd.concat([ser, data], axis=1)\n",
    "        df_joined = df.join(self._bin_mappings, 'labels', rsuffix=self._suffix)\n",
    "\n",
    "        return df_joined[data.name + self._suffix]\n",
    "\n",
    "    def fit_transform(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Train with then return the transformation of some data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ser, bins = pd.qcut(data, self._quantiles, retbins=True, labels=False)\n",
    "        ser.name = 'labels'\n",
    "\n",
    "        df = pd.concat([ser, data], axis=1)\n",
    "        bin_mappings = df.groupby('labels').mean()\n",
    "        df_joined = df.join(bin_mappings, 'labels', rsuffix=self._suffix)\n",
    "\n",
    "        self._bins = bins\n",
    "        self._bin_mappings = bin_mappings\n",
    "\n",
    "        return df_joined[data.name + self._suffix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plugIn(msg, w):\n",
    "    # Compute plug-in (ML) entropy rate\n",
    "    pmf = pmf1(msg, w)\n",
    "    out = - sum([pmf[i] * np.log2(pmf[i]) for i in pmf]) / w\n",
    "    return out, pmf\n",
    "\n",
    "def pmf1(msg, w):\n",
    "    # Compute the prob mass function for a 1D discrete RV\n",
    "    # len(msg)-w occurances\n",
    "    lib = {}\n",
    "    if not isinstance(msg, str): msg = ''.join(map(str, msg))\n",
    "    for i in range(w, len(msg)):\n",
    "        msg_ = msg[i-w: i]\n",
    "        if msg_ not in lib: \n",
    "            lib[msg_] = [i-w]\n",
    "        else: \n",
    "            lib[msg_] = lib[msg_] + [i-w]\n",
    "    pmf = float(len(msg) - w)\n",
    "    pmf = {i: len(lib[i])/pmf for i in lib}\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: 01100001, entropy: 0.84\n"
     ]
    }
   ],
   "source": [
    "class EntropyEstimatorLz:\n",
    "    \"\"\"\n",
    "    Kontoyiannis' LZ entropy estimate, 2013 version (centered window). Inverse\n",
    "    of the avg length of the shortest non-redundant substring. If non-redundant\n",
    "    substrings are short, the text is highly entropic. window==None for\n",
    "    expanding window, in which case\n",
    "    len(msg) % 2 == 0\n",
    "    If the end of msg is more relevant, try estimate_entropy(msg[::-1])\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def estimate_entropy(cls, *args, **kwargs):\n",
    "        return cls.konto(*args, **kwargs)['h']\n",
    "\n",
    "    @classmethod\n",
    "    def konto(cls, msg, window=None):\n",
    "        \"\"\"\n",
    "        :param msg:\n",
    "        :param window:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = {'num': 0, 'sum': 0, 'sub_str': []}\n",
    "        if not isinstance(msg, str):\n",
    "            msg = ''.join(map(str, msg))\n",
    "\n",
    "        if window is None:\n",
    "            points = range(1, len(msg) // 2 + 1)\n",
    "\n",
    "        else:\n",
    "            window = min(window, len(msg) // 2)\n",
    "            points = range(window, len(msg) - window + 1)\n",
    "\n",
    "        for i in points:\n",
    "            if window is None:\n",
    "                l, msg_ = cls.match_length(msg, i, i)\n",
    "                out['sum'] += math.log2(i + 1) / l\n",
    "\n",
    "            else:\n",
    "                l, msg_ = cls.match_length(msg, i, window)\n",
    "                out['sum'] += math.log2(window + 1) / l\n",
    "\n",
    "            out['sub_str'].append(msg_)\n",
    "            out['num'] += 1\n",
    "\n",
    "        out['h'] = (out['sum'] / out['num']) / math.log(2)\n",
    "        out['r'] = 1 - out['h'] / math.log2(len(msg))  # redundancy, 0 <= r <= 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def match_length(msg, i, n):\n",
    "        \"\"\"\n",
    "        Maximum matched length + 1, with overlap.\n",
    "        i >= n & len(msg) >= i + n\n",
    "        :param msg:\n",
    "        :param i:\n",
    "        :param n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sub_str = ''\n",
    "        for l in range(n):\n",
    "            msg1 = msg[i:i + l + 1]\n",
    "\n",
    "            for j in range(i - n, i):\n",
    "                msg0 = msg[j:j + l + 1]\n",
    "\n",
    "                if msg1 == msg0:\n",
    "                    sub_str = msg1\n",
    "                    break  # search for higher l.\n",
    "\n",
    "        return len(sub_str) + 1, sub_str  # matched length + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Messages produces entropies of 0.97 and 0.84 as highlighted in\n",
    "    # \"Advances in Financial Machine Learning\" section 18.4\n",
    "    for m in ('11100001', '01100001'):\n",
    "        h = EntropyEstimatorLz.estimate_entropy(m) * math.log(2)\n",
    "print('message: %s, entropy: %.2f' % (m, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(total):\n",
    "    current_seg = []\n",
    "    len_seg = len(total)//1000\n",
    "    for item in total:\n",
    "        if len(current_seg) < len_seg:\n",
    "            current_seg.append(item)\n",
    "            continue\n",
    "        yield current_seg\n",
    "        current_seg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "df = get_ohlcv_poloniex(pair='USDT_BTC', start=0, end=9999999999, period=900)\n",
    "#df['sumVolume'] = 0\n",
    "sumVolume = 0\n",
    "volumeBars = pd.DataFrame()\n",
    "for i, row in df.iterrows():\n",
    "    sumVolume += row['quoteVolume']\n",
    "    if sumVolume >= 100:\n",
    "        volumeBars = volumeBars.append(row)\n",
    "        sumVolume = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumeBars['ret'] = volumeBars['weightedAverage'].pct_change()\n",
    "volumeBars = volumeBars[volumeBars['ret'] != 0]\n",
    "volumeBars = volumeBars[volumeBars['ret'] != np.inf]\n",
    "volumeBars = volumeBars[volumeBars['ret'].notnull()]\n",
    "volumeBars['binary'] = volumeBars.apply(lambda row: int(row['ret']*(row['ret']-1) > 0), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "partitions = segment(volumeBars['binary'])\n",
    "num_segments = len(volumeBars['binary'])//1000\n",
    "print(num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plug in entropies\n"
     ]
    }
   ],
   "source": [
    "print(\"Plug in entropies\")\n",
    "# for part in partitions:\n",
    "#     print(part)\n",
    "#     print(str(plugIn(part, 10)[0]))\n",
    "results = pd.DataFrame()\n",
    "results['plug-in'] = [plugIn(part, 10)[0] for part in partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kontoyiannis’ method entropies using window size of 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Kontoyiannis’ method entropies using window size of 100\")\n",
    "partitions = segment(volumeBars['binary'])\n",
    "# for part in partitions:\n",
    "#     print(part)\n",
    "#     print(\"binary: \" + str(EntropyEstimatorLz.estimate_entropy(part, window=100)))\n",
    "results['kontoyiannis'] = [EntropyEstimatorLz.estimate_entropy(part, window=100) for part in partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1851223729162324\n"
     ]
    }
   ],
   "source": [
    "print(results['kontoyiannis'].corr(results['plug-in']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
